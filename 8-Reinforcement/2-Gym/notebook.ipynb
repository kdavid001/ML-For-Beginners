{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "interpreter": {
   "hash": "86193a1ab0ba47eac1c69c1756090baa3b420b3eea7d4aafab8b85f8b312f0c5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## CartPole Skating\n",
    "\n",
    "> **Problem**: If Peter wants to escape from the wolf, he needs to be able to move faster than him. We will see how Peter can learn to skate, in particular, to keep balance, using Q-Learning.\n",
    "\n",
    "First, let's install the gym and import required libraries:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:25:36.743781Z",
     "start_time": "2025-05-09T12:25:35.260685Z"
    }
   },
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install gym \n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "source": [
    "## Create a cartpole environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#code block 2\n",
    "env = gym.make(\"CartPole-v1\", render_mode= 'human')\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.action_space.sample())"
   ],
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:25:36.758951Z",
     "start_time": "2025-05-09T12:25:36.746294Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "source": [
    "To see how the environment works, let's run a short simulation for 100 steps."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# env.reset()\n",
    "# \n",
    "# done = False\n",
    "# while not done:\n",
    "#    env.render()\n",
    "#    obs, rew, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "#    done = terminated or truncated\n",
    "#    print(f\"{obs} -> {rew}\")\n",
    "# env.close()"
   ],
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:25:36.763578Z",
     "start_time": "2025-05-09T12:25:36.760740Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "source": [
    "During simulation, we need to get observations in order to decide how to act. In fact, `step` function returns us back current observations, reward function, and the `done` flag that indicates whether it makes sense to continue the simulation or not:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "",
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:25:36.768460Z",
     "start_time": "2025-05-09T12:25:36.766022Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "source": [
    "We can get min and max value of those numbers:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:25:36.774829Z",
     "start_time": "2025-05-09T12:25:36.771903Z"
    }
   },
   "source": [
    "#code block 5"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "source": [
    "## State Discretization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:25:36.779033Z",
     "start_time": "2025-05-09T12:25:36.775825Z"
    }
   },
   "source": [
    "def discretize(x):\n",
    "    return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(int))"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "source": [
    "Let's also explore other discretization method using bins:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:25:36.784908Z",
     "start_time": "2025-05-09T12:25:36.780045Z"
    }
   },
   "source": [
    "def create_bins(i,num):\n",
    "    return np.arange(num+1)*(i[1]-i[0])/num+i[0]\n",
    "\n",
    "print(\"Sample bins for interval (-5,5) with 10 bins\\n\",create_bins((-5,5),10))\n",
    "\n",
    "ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter\n",
    "nbins = [20,20,10,10] # number of bins for each parameter\n",
    "bins = [create_bins(ints[i],nbins[i]) for i in range(4)]\n",
    "\n",
    "def discretize_bins(x):\n",
    "    return tuple(np.digitize(x[i],bins[i]) for i in range(4))#code block 7"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "source": [
    "Let's now run a short simulation and observe those discrete environment values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:25:36.788903Z",
     "start_time": "2025-05-09T12:25:36.786191Z"
    }
   },
   "source": [
    "# env.reset()\n",
    "# \n",
    "# done = False\n",
    "# while not done:\n",
    "#    #env.render()\n",
    "#    obs, rew, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "#    done = terminated or truncated   #print(discretize_bins(obs))\n",
    "#    # print(discretize(obs))\n",
    "# env.close()#code block 8"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "source": [
    "## Q-Table Structure"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:25:36.792814Z",
     "start_time": "2025-05-09T12:25:36.789885Z"
    }
   },
   "source": [
    "Q = {}\n",
    "actions = (0,1)\n",
    "\n",
    "def qvalues(state):\n",
    "    return [Q.get((state,a),0) for a in actions]"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "source": [
    "## Let's Start Q-Learning!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:25:36.795979Z",
     "start_time": "2025-05-09T12:25:36.793797Z"
    }
   },
   "source": [
    "# hyperparameters\n",
    "alpha = 0.3\n",
    "gamma = 0.9\n",
    "epsilon = 0.90#code block 10"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T12:32:40.658497Z",
     "start_time": "2025-05-09T12:25:36.796927Z"
    }
   },
   "source": [
    "def probs(v,eps=1e-4):\n",
    "    v = v-v.min()+eps\n",
    "    v = v/v.sum()\n",
    "    return v\n",
    "\n",
    "Qmax = 0\n",
    "cum_rewards = []\n",
    "rewards = []\n",
    "for epoch in range(100000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    cum_reward=0\n",
    "    # == do the simulation ==\n",
    "    while not done:\n",
    "        s = discretize(obs)\n",
    "        if random.random()<epsilon:\n",
    "            # exploitation - chose the action according to Q-Table probabilities\n",
    "            v = probs(np.array(qvalues(s)))\n",
    "            a = random.choices(actions,weights=v)[0]\n",
    "        else:\n",
    "            # exploration - randomly chose the action\n",
    "            a = np.random.randint(env.action_space.n)\n",
    "\n",
    "        obs, rew, terminated, truncated, _ = env.step(a)\n",
    "        done = terminated or truncated\n",
    "        cum_reward+=rew\n",
    "        ns = discretize(obs)\n",
    "        Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))\n",
    "    cum_rewards.append(cum_reward)\n",
    "    rewards.append(cum_reward)\n",
    "    # == Periodically print results and calculate average reward ==\n",
    "    if epoch%5000==0:\n",
    "        print(f\"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}\")\n",
    "        if np.average(cum_rewards) > Qmax:\n",
    "            Qmax = np.average(cum_rewards)\n",
    "            Qbest = Q\n",
    "        cum_rewards=[]#code block 11"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "source": [
    "## Plotting Training Progress"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.plot(rewards)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "From this graph, it is not possible to tell anything, because due to the nature of stochastic training process the length of training sessions varies greatly. To make more sense of this graph, we can calculate **running average** over series of experiments, let's say 100. This can be done conveniently using `np.convolve`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#code block 12"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Varying Hyperparameters and Seeing the Result in Action\n",
    "\n",
    "Now it would be interesting to actually see how the trained model behaves. Let's run the simulation, and we will be following the same action selection strategy as during training: sampling according to the probability distribution in Q-Table: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# code block 13"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "\n",
    "## Saving result to an animated GIF\n",
    "\n",
    "If you want to impress your friends, you may want to send them the animated GIF picture of the balancing pole. To do this, we can invoke `env.render` to produce an image frame, and then save those to animated GIF using PIL library:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "obs = env.reset()\n",
    "done = False\n",
    "i=0\n",
    "ims = []\n",
    "while not done:\n",
    "   s = discretize(obs)\n",
    "   img=env.render(mode='rgb_array')\n",
    "   ims.append(Image.fromarray(img))\n",
    "   v = probs(np.array([Qbest.get((s,a),0) for a in actions]))\n",
    "   a = random.choices(actions,weights=v)[0]\n",
    "   obs,_,done,_ = env.step(a)\n",
    "   i+=1\n",
    "env.close()\n",
    "ims[0].save('images/cartpole-balance.gif',save_all=True,append_images=ims[1::2],loop=0,duration=5)\n",
    "print(i)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
