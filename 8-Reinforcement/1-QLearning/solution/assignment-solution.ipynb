{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('3.7')"
  },
  "interpreter": {
   "hash": "70b38d7a306a849643e446cd70466270a13445e5987dfa1344ef2b127438fa4d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Peter and the Wolf: Realistic Environment\n",
    "\n",
    "In our situation, Peter was able to move around almost without getting tired or hungry. In more realistic world, we has to sit down and rest from time to time, and also to feed himself. Let's make our world more realistic, by implementing the following rules:\n",
    "\n",
    "1. By moving from one place to another, Peter loses **energy** and gains some **fatigue**.\n",
    "2. Peter can gain more energy by eating apples.\n",
    "3. Peter can get rid of fatigue by resting under the tree or on the grass (i.e. walking into a board location with a tree or grass - green field)\n",
    "4. Peter needs to find and kill the wolf\n",
    "5. In order to kill the wolf, Peter needs to have certain levels of energy and fatigue, otherwise he loses the battle.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from rlboard import *"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "width, height = 8,8\n",
    "m = Board(width,height)\n",
    "m.randomize(seed=13)\n",
    "m.plot()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "actions = { \"U\" : (0,-1), \"D\" : (0,1), \"L\" : (-1,0), \"R\" : (1,0) }\n",
    "action_idx = { a : i for i,a in enumerate(actions.keys()) }"
   ],
   "outputs": []
  },
  {
   "source": [
    "## Defining state\n",
    "\n",
    "In our new game rules, we need to keep track of energy and fatigue at each board state. Thus we will create an object `state` that will carry all required information about current problem state, including state of the board, current levels of energy and fatigue, and whether we can win the wolf while at terminal state:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "class state:\n",
    "    def __init__(self,board,energy=10,fatigue=0,init=True):\n",
    "        self.board = board\n",
    "        self.energy = energy\n",
    "        self.fatigue = fatigue\n",
    "        self.dead = False\n",
    "        if init:\n",
    "            self.board.random_start()\n",
    "        self.update()\n",
    "\n",
    "    def at(self):\n",
    "        return self.board.at()\n",
    "\n",
    "    def update(self):\n",
    "        if self.at() == Board.Cell.water:\n",
    "            self.dead = True\n",
    "            return\n",
    "        if self.at() == Board.Cell.tree:\n",
    "            self.fatigue = 0\n",
    "        if self.at() == Board.Cell.apple:\n",
    "            self.energy = 10\n",
    "\n",
    "    def move(self,a):\n",
    "        self.board.move(a)\n",
    "        self.energy -= 1\n",
    "        self.fatigue += 1\n",
    "        self.update()\n",
    "\n",
    "    def is_winning(self):\n",
    "        return self.energy > self.fatigue"
   ],
   "outputs": []
  },
  {
   "source": [
    "Let's try to solve the problem using random walk and see if we succeed:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "source": [
    "def random_policy(state):\n",
    "    return random.choice(list(actions))\n",
    "\n",
    "def walk(board,policy):\n",
    "    n = 0 # number of steps\n",
    "    s = state(board)\n",
    "    while True:\n",
    "        if s.at() == Board.Cell.wolf:\n",
    "            if s.is_winning():\n",
    "                return n # success!\n",
    "            else:\n",
    "                return -n # failure!\n",
    "        if s.at() == Board.Cell.water:\n",
    "            return 0 # died\n",
    "        a = actions[policy(m)]\n",
    "        s.move(a)\n",
    "        n+=1\n",
    "\n",
    "walk(m,random_policy)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "def print_statistics(policy):\n",
    "    s,w,n = 0,0,0\n",
    "    for _ in range(100):\n",
    "        z = walk(m,policy)\n",
    "        if z<0:\n",
    "            w+=1\n",
    "        elif z==0:\n",
    "            n+=1\n",
    "        else:\n",
    "            s+=1\n",
    "    print(f\"Killed by wolf = {w}, won: {s} times, drown: {n} times\")\n",
    "\n",
    "print_statistics(random_policy)"
   ],
   "outputs": []
  },
  {
   "source": [
    "## Reward Function\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "def reward(s):\n",
    "    r = s.energy-s.fatigue\n",
    "    if s.at()==Board.Cell.wolf:\n",
    "        return 100 if s.is_winning() else -100\n",
    "    if s.at()==Board.Cell.water:\n",
    "        return -100\n",
    "    return r"
   ],
   "outputs": []
  },
  {
   "source": [
    "## Q-Learning algorithm\n",
    "\n",
    "The actual learning algorithm stays pretty much unchanged, we just use `state` instead of just board position."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "def probs(v,eps=1e-4):\n",
    "    v = v-v.min()+eps\n",
    "    v = v/v.sum()\n",
    "    return v"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "lpath = []\n",
    "\n",
    "for epoch in range(10000):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Epoch = {epoch}\",end='')\n",
    "\n",
    "    # Pick initial point\n",
    "    s = state(m)\n",
    "    \n",
    "    # Start travelling\n",
    "    n=0\n",
    "    cum_reward = 0\n",
    "    while True:\n",
    "        x,y = s.board.human\n",
    "        v = probs(Q[x,y])\n",
    "        while True:\n",
    "            a = random.choices(list(actions),weights=v)[0]\n",
    "            dpos = actions[a]\n",
    "            if s.board.is_valid(s.board.move_pos(s.board.human,dpos)):\n",
    "                break \n",
    "        s.move(dpos)\n",
    "        r = reward(s)\n",
    "        if abs(r)==100: # end of game\n",
    "            print(f\" {n} steps\",end='\\r')\n",
    "            lpath.append(n)\n",
    "            break\n",
    "        alpha = np.exp(-n / 3000)\n",
    "        gamma = 0.5\n",
    "        ai = action_idx[a]\n",
    "        Q[x,y,ai] = (1 - alpha) * Q[x,y,ai] + alpha * (r + gamma * Q[x+dpos[0], y+dpos[1]].max())\n",
    "        n+=1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "m.plot(Q)"
   ],
   "outputs": []
  },
  {
   "source": [
    "## Results\n",
    "\n",
    "Let's see if we were successful training Peter to fight the wolf!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "def qpolicy(m):\n",
    "        x,y = m.human\n",
    "        v = probs(Q[x,y])\n",
    "        a = random.choices(list(actions),weights=v)[0]\n",
    "        return a\n",
    "\n",
    "print_statistics(qpolicy)"
   ],
   "outputs": []
  },
  {
   "source": [
    "We now see much less cases of drowning, but Peter is still not always able to kill the wolf. Try to experiment and see if you can improve this result by playing with hyperparameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "plt.plot(lpath)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ]
}
