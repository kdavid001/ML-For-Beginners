{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('3.7')"
  },
  "interpreter": {
   "hash": "70b38d7a306a849643e446cd70466270a13445e5987dfa1344ef2b127438fa4d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Peter and the Wolf: Reinforcement Learning Primer\n",
    "\n",
    "In this tutorial, we will learn how to apply Reinforcement learning to a problem of path finding. The setting is inspired by [Peter and the Wolf](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) musical fairy tale by Russian composer [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). It is a story about young pioneer Peter, who bravely goes out of his house to the forest clearing to chase a wolf. We will train machine learning algorithms that will help Peter to explore the surrounding area and build an optimal navigation map.\n",
    "\n",
    "First, let's import a bunch of useful libraries:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ],
   "outputs": []
  },
  {
   "source": [
    "## Overview of Reinforcement Learning\n",
    "\n",
    "**Reinforcement Learning** (RL) is a learning technique that allows us to learn an optimal behaviour of an **agent** in some **environment** by running many experiments. An agent in this environment should have some **goal**, defined by a **reward function**.\n",
    "\n",
    "## The Environment\n",
    "\n",
    "For simplicity, let's consider Peter's world to be a square board of size `width` x `height`. Each cell in this board can either be:\n",
    "* **ground**, on which Peter and other creatures can walk\n",
    "* **water**, on which you obviously cannot walk\n",
    "* **a tree** or **grass** - a place where you can rest\n",
    "* **an apple**, which represents something Peter would be glad to find in order to feed himself\n",
    "* **a wolf**, which is dangerous and should be avoided\n",
    "\n",
    "To work with the environment, we will define a class called `Board`. In order not to clutter this notebook too much, we have moved all code to work with the board into separate `rlboard` module, which we will now import. You may look inside this module to get more details about the internals of the implementation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's now create a random board and see how it looks:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# code block 1"
   ],
   "outputs": []
  },
  {
   "source": [
    "## Actions and Policy\n",
    "\n",
    "In our example, Peter's goal would be to find an apple, while avoiding the wolf and other obstacles. Define those actions as a dictionary, and map them to pairs of corresponding coordinate changes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# code block 2"
   ],
   "outputs": []
  },
  {
   "source": [
    "The strategy of our agent (Peter) is defined by a so-called **policy**. Let's consider the simplest policy called **random walk**.\n",
    "\n",
    "## Random walk\n",
    "\n",
    "Let's first solve our problem by implementing a random walk strategy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Let's run a random walk experiment several times and see the average number of steps taken: code block 3"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# code block 4"
   ],
   "outputs": []
  },
  {
   "source": [
    "## Reward Function\n",
    "\n",
    "To make our policy more intelligent, we need to understand which moves are \"better\" than others.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "#code block 5"
   ],
   "outputs": []
  },
  {
   "source": [
    "## Q-Learning\n",
    "\n",
    "Build a Q-Table, or multi-dimensional array. Since our board has dimensions `width` x `height`, we can represent Q-Table by a numpy array with shape `width` x `height` x `len(actions)`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "# code block 6"
   ],
   "outputs": []
  },
  {
   "source": [
    "Pass the Q-Table to the `plot` function in order to visualize the table on the board:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "m.plot(Q)"
   ],
   "outputs": []
  },
  {
   "source": [
    "\n",
    "## Essence of Q-Learning: Bellman Equation and  Learning Algorithm\n",
    "\n",
    "Write a pseudo-code for our leaning algorithm:\n",
    "\n",
    "* Initialize Q-Table Q with equal numbers for all states and actions\n",
    "* Set learning rate $\\alpha\\leftarrow 1$\n",
    "* Repeat simulation many times\n",
    "   1. Start at random position\n",
    "   1. Repeat\n",
    "        1. Select an action $a$ at state $s$\n",
    "        2. Exectute action by moving to a new state $s'$\n",
    "        3. If we encounter end-of-game condition, or total reward is too small - exit simulation  \n",
    "        4. Compute reward $r$ at the new state\n",
    "        5. Update Q-Function according to Bellman equation: $Q(s,a)\\leftarrow (1-\\alpha)Q(s,a)+\\alpha(r+\\gamma\\max_{a'}Q(s',a'))$\n",
    "        6. $s\\leftarrow s'$\n",
    "        7. Update total reward and decrease $\\alpha$.\n",
    "\n",
    "## Exploit vs. Explore\n",
    "\n",
    "The best approach is to balance between exploration and exploitation. As we learn more about our environment, we would be more likely to follow the optimal route, however, choosing the unexplored path once in a while.\n",
    "\n",
    "## Python Implementation\n",
    "\n",
    "Now we are ready to implement the learning algorithm. Before that, we also need some function that will convert arbitrary numbers in the Q-Table into a vector of probabilities for corresponding actions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "# code block 7"
   ],
   "outputs": []
  },
  {
   "source": [
    "We add a small amount of `eps` to the original vector in order to avoid division by 0 in the initial case, when all components of the vector are identical.\n",
    "\n",
    "The actual learning algorithm we will run for 5000 experiments, also called **epochs**: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "source": [
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "lpath = []\n",
    "\n",
    "# code block 8"
   ],
   "outputs": []
  },
  {
   "source": [
    "After executing this algorithm, the Q-Table should be updated with values that define the attractiveness of different actions at each step. Visualize the table here:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "source": [
    "m.plot(Q)"
   ],
   "outputs": []
  },
  {
   "source": [
    "## Checking the Policy\n",
    "\n",
    "Since Q-Table lists the \"attractiveness\" of each action at each state, it is quite easy to use it to define the efficient navigation in our world. In the simplest case, we can just select the action corresponding to the highest Q-Table value:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "# code block 9"
   ],
   "outputs": []
  },
  {
   "source": [
    "If you try the code above several times, you may notice that sometimes it just \"hangs\", and you need to press the STOP button in the notebook to interrupt it. \n",
    "\n",
    "> **Task 1:** Modify the `walk` function to limit the maximum length of path by a certain number of steps (say, 100), and watch the code above return this value from time to time.\n",
    "\n",
    "> **Task 2:** Modify the `walk` function so that it does not go back to the places where is has already been previously. This will prevent `walk` from looping, however, the agent can still end up being \"trapped\" in a location from which it is unable to escape."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "source": [
    "\n",
    "# code block 10"
   ],
   "outputs": []
  },
  {
   "source": [
    "## Investigating the Learning Process"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "source": [
    "plt.plot(lpath)"
   ],
   "outputs": []
  },
  {
   "source": [
    "## Exercise\n",
    "## A more realistic Peter and the Wolf world\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
